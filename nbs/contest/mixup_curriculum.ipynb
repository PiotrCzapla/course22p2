{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Chris 94.9 on 5ep (not reporduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed mods\n",
    "from miniai.utils import *\n",
    "\n",
    "MetricsCB = LazyMetricsCB\n",
    "ProgressCB = LazyProgressCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "from torch import distributions\n",
    "\n",
    "torch.set_printoptions(precision=8, linewidth=140, sci_mode=False)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        dist = distributions.binomial.Binomial(tensor(1.0).to(x.device), probs=1-self.p)\n",
    "        return x * dist.sample(x.size()) * 1/(1-self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n",
    "#https://arxiv.org/abs/1908.08681v1\n",
    "#implemented for PyTorch / FastAI by lessw2020 \n",
    "#github: https://github.com/lessw2020/mish\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n",
    "        return x *( torch.tanh(F.softplus(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "class TopLossesCallback(Callback):\n",
    "  epoch_preds = []\n",
    "  epoch_targets = []\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def _calculate_top_losses(self):\n",
    "    preds = torch.cat(self.epoch_preds, dim=0)\n",
    "    targets = torch.cat(self.epoch_targets, dim=0)\n",
    "    losses = F.cross_entropy(preds, targets, reduce=False)\n",
    "    return torch.topk(losses, preds.shape[0]).indices\n",
    "\n",
    "  def after_batch(self, learn):\n",
    "    if not learn.model.training:\n",
    "      return\n",
    "    self.epoch_preds.append(learn.preds)\n",
    "    self.epoch_targets.append(learn.batch[1])\n",
    "\n",
    "  def before_epoch(self, learn):\n",
    "    self.epoch_preds = []\n",
    "    self.epoch_targets = []\n",
    "\n",
    "  def after_epoch(self, learn):\n",
    "    if not learn.model.training:\n",
    "      return\n",
    "    learn.dls.train.sampler.top_losses = self._calculate_top_losses()\n",
    "\n",
    "# tweaked from tommyc's version\n",
    "# Before certain epoch drop a % of the training dataset with the lowest losses.\n",
    "# Replace them with the the same % of the training dataset with the highest losses.\n",
    "# This gives the model two opportunities to train on the most challenging images.\n",
    "class CustomTrainingSampler(WeightedRandomSampler):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    WeightedRandomSampler.__init__(self, *args, **kwargs)\n",
    "    self.data_indexes_for_epoch = []\n",
    "    self.top_losses = []\n",
    "    self.epoch = -1\n",
    "    self.n = {\n",
    "        0: 0,\n",
    "        1: 0.21,\n",
    "        2: 0.42,\n",
    "        3: 0.21,\n",
    "        4: 0\n",
    "    }\n",
    "\n",
    "  def __iter__(self) -> Iterator[int]:\n",
    "      self.epoch += 1\n",
    "      rand_tensor = torch.randperm(self.num_samples, generator=self.generator).tolist()\n",
    "      n = int(self.n[self.epoch] * self.num_samples)\n",
    "\n",
    "      if n != 0:\n",
    "        # TODO: Cleanup the code below\n",
    "        inverted_losses_for_epochs = torch.flip(torch.tensor(self.top_losses.clone().detach()[:n]), dims=(0,)).cpu()\n",
    "        self.data_indexes_for_epoch = torch.tensor(self.data_indexes_for_epoch).cpu()\n",
    "        self.data_indexes_for_epoch[self.top_losses[-n:].cpu()] = self.data_indexes_for_epoch[inverted_losses_for_epochs]\n",
    "        self.data_indexes_for_epoch = self.data_indexes_for_epoch[rand_tensor]\n",
    "        self.data_indexes_for_epoch = self.data_indexes_for_epoch.tolist()\n",
    "      else:\n",
    "        self.data_indexes_for_epoch = rand_tensor\n",
    "\n",
    "      yield from self.data_indexes_for_epoch\n",
    "\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, *dls): \n",
    "      self.train,self.valid = dls[:2]\n",
    "\n",
    "    def get_sampler(num_samples, mode=\"train\"):\n",
    "      if mode != \"train\":\n",
    "        return None\n",
    "      return CustomTrainingSampler(weights=[1 for _ in range(num_samples)], num_samples=num_samples)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "      return cls(*[DataLoader(ds, batch_size, sampler=cls.get_sampler(len(ds), mode), collate_fn=collate_dict(ds), **kwargs) for mode, ds in dd.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e915c428f48a88d49f1059e10616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 256\n",
    "xmean,xstd = 0.28, 0.35\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds.cached = cache_dataset_as_dict(tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model9(act=nn.ReLU, nfs=(32,288,288,288,288,288), norm=nn.BatchNorm2d):#,256\n",
    "    layers = [ResBlock(1, 32, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(0.65))\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CapturePreds(Callback):\n",
    "    def before_fit(self, learn): self.all_preds,self.all_targs = [],[]\n",
    "    def after_batch(self, learn):\n",
    "        self.all_preds.append(to_cpu(learn.preds))\n",
    "        self.all_targs.append(to_cpu(learn.batch[1]))\n",
    "    def after_fit(self, learn): self.all_preds,self.all_targs = torch.cat(self.all_preds),torch.cat(self.all_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "def capture_preds(self: Learner, cbs=None):\n",
    "    cp = CapturePreds()\n",
    "    self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n",
    "    return cp.all_preds,cp.all_targs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dd(tds.cached, bs, num_workers=0)\n",
    "\n",
    "# tweaked from rohitgeo's version\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics] \n",
    "#0.0003 from https://github.com/digantamisra9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_cb(scale, mode='bilinear'): \n",
    "    return BatchTransformCB(lambda b: (F.interpolate(b[0], scale_factor=scale, mode=mode),b[1]),\n",
    "                            on_val=True, on_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = rng_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, leaky=0.0003, seed=1, m=1, cbs=tuple(), fit=True, train_cb=TrainCB(), epochs=5, base_lr=2e-2, \n",
    "        loss_func=F.cross_entropy, bs=bs, tta=False, dls=None, verbose=True):\n",
    "    rng.set_seed(seed)\n",
    "    if verbose: print(torch.randn([3]))\n",
    "    iw = partial(init_weights, leaky=leaky) if leaky is not None else fc.noop\n",
    "    lr = base_lr*m\n",
    "    if verbose: print(\"Batch size\", bs*m)\n",
    "    dls = dls or DataLoaders.from_dd(tds, bs*m, num_workers=0) \n",
    "    tmax = epochs * len(dls.train)\n",
    "    sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "    cbs = [DeviceCB(), rng_seed, metrics, BatchSchedCB(sched), *cbs, train_cb] \n",
    "    if verbose: print(torch.randn([3]))\n",
    "    model=model.apply(iw)\n",
    "    learn = Learner(model, dls, loss_func, lr=lr, cbs=cbs, opt_func=optim.AdamW)\n",
    "    if verbose: print(torch.randn([3]))\n",
    "    if verbose: print(next(iter(learn.dls.train))[1])\n",
    "    if fit:\n",
    "        learn.fit(epochs, cbs=[TimeItCB(), ProgressCB(plot=True)])\n",
    "    if tta:\n",
    "        ## TTA\n",
    "        ap1, at = learn.capture_preds()\n",
    "        ttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)\n",
    "        ap2, at = learn.capture_preds(cbs=[ttacb])\n",
    "        ap = torch.stack([ap1,ap2]).mean(0).argmax(1)\n",
    "        if verbose: print('TTA:', round((ap==at).float().mean().item(), 4))\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MixUp v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamic mixup loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = fc.first(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yab = torch.tensor([1.,0.,0.,1.])\n",
    "s = torch.tensor([0.1,0.2,0.3,0.4])\n",
    "\n",
    "#yab.T @ s @ yab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "def mixup_data(b2, b1=None, sampler=torch.distributions.Beta(tensor(0.5), tensor(0.5)).sample, classes=10, permute_1=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    r = SimpleNamespace()\n",
    "    if b1 is None: b1=b2 # first batch uses it self\n",
    "    x1,y1 = b1\n",
    "    x2,y2 = b2\n",
    "    \n",
    "    if x1.shape[0] != x2.shape[0]: x1,y1=x2,y2 # last batch uses it self\n",
    "    \n",
    "    # permute prev batch\n",
    "    batch_size = x2.shape[0]\n",
    "    if permute_1:\n",
    "        index = torch.randperm(batch_size).to(x2.device)\n",
    "        x1,y1 = x1[index], y1[index]\n",
    "    \n",
    "    r.lam = sampler([batch_size]).to(x2.device)\n",
    "    r.mixed_x = torch.lerp(x2, x1, r.lam.reshape(-1,*[1]*(len(x2.shape)-1)))\n",
    "    \n",
    "    r.y1 = F.one_hot(y1, num_classes=classes)\n",
    "    r.y2 = F.one_hot(y2, num_classes=classes)\n",
    "    r.mixed_y = r.y2 + r.lam.reshape(-1,1)*(r.y1-r.y2)\n",
    "    #mixed_y = torch.lerp(y2, y2, lam.reshape(-1,1))\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = mixup_data((xb,yb))\n",
    "show_images(r.mixed_x[:16], imsize=1.5)\n",
    "r.lam[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "b = 20\n",
    "a = 10.\n",
    "x1 = tensor([[1,  1, a, 1],\n",
    "             [a,  1, 1, 1]]).float()\n",
    "x2 = tensor([[1,  b, 1, 1],\n",
    "             [1,  1, 1, b]]).float()\n",
    "y1 = (x1).argmax(-1)\n",
    "y2 = (x2).argmax(-1) \n",
    "\n",
    "print(y1, y2)\n",
    "r = mixup_data((x1,y1), (x2,y2), classes=4, permute_1=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.73 * b+ 0.27*1), 0.2689*a+ 0.73*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets construct the mask manually\n",
    "maskb = r.mixed_x>14\n",
    "maska = (r.mixed_x>2) & (~maskb)\n",
    "maska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=r.mixed_x[~maskb].reshape(2, -1) \n",
    "lbl=maska[~maskb].reshape(2, -1).float()\n",
    "print(pred,lbl)\n",
    "F.cross_entropy(pred, lbl, reduction='mean').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_masked(preds, y, ignored_y, **kwargs):\n",
    "    \"\"\"\n",
    "    y - one hot encoded label\n",
    "    ingored_y - one hot encloded of ignored class\n",
    "    \"\"\"\n",
    "    N, C = preds.shape\n",
    "    mask = ignored_y==0\n",
    "    mpreds = preds[mask].reshape(N, C-1) # to fail early\n",
    "    my = y[mask].float().reshape(N, C-1)\n",
    "    return F.cross_entropy(mpreds, my, **kwargs)\n",
    "\n",
    "ce_masked(r.mixed_x, r.y2, r.y1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_masked(r.mixed_x, r.y1, r.y1).item() + ce_masked(r.mixed_y, r.y2, r.y2).item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Westlake-AI/openmixup/blob/c042813ee0af577d365f0e13b13a4c8486d6e8f7/openmixup/models/losses/cross_entropy_loss.py#L83\n",
    "\n",
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss tensor.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch. Options are \"none\",\n",
    "            \"mean\" and \"sum\".\n",
    "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        # none: 0, elementwise_mean:1, sum: 2\n",
    "        if reduction_enum == 1:\n",
    "            loss = loss.mean()\n",
    "        elif reduction_enum == 2:\n",
    "            loss = loss.sum()\n",
    "    else:\n",
    "        # if reduction is 'mean', then average the loss by avg_factor\n",
    "        if reduction_enum == 1:\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing; otherwise raise an error\n",
    "        elif reduction != 0:\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss\n",
    "\n",
    "def soft_mix_cross_entropy(pred,\n",
    "                           label,\n",
    "                           weight=None,\n",
    "                           reduction='mean',\n",
    "                           class_weight=None,\n",
    "                           avg_factor=None,\n",
    "                           eta_weight=None,\n",
    "                           eps_smooth=1e-3,\n",
    "                           verbose=False,\n",
    "                           **kwargs):\n",
    "    r\"\"\"Calculate the Soft Decoupled Mixup CrossEntropy loss using softmax\n",
    "        The label can be float mixup label (class-wise sum to 1, k-mixup, k>=2).\n",
    "       *** Warnning: this mixup and label-smoothing cannot be set simultaneously ***\n",
    "    Decoupled Mixup for Data-efficient Learning. In arXiv, 2022.\n",
    "    <https://arxiv.org/abs/2203.10761>\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n",
    "            of classes.\n",
    "        label (torch.Tensor): The gt label of the prediction with shape (N, C).\n",
    "            When using \"mixup\", the label can be float (mixup one-hot label).\n",
    "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "        reduction (str): The method used to reduce the loss.\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (torch.Tensor, optional): The weight for each class with\n",
    "            shape (C), C is the number of classes. Default None.\n",
    "        eta_weight (list): Reweight the global loss in mixup cls loss as,\n",
    "            loss = loss_local + eta_weight[i] * loss_global[i]. Default to None.\n",
    "        eps_smooth (float): If using label smoothing, we assume eps < lam < 1-eps.\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    \"\"\"\n",
    "    # *** Assume k-mixup in C classes, k >= 2 and k << C ***\n",
    "    # step 1: remove labels have less than k-hot (mixed between the\n",
    "    #    same class will result in the original onehot)\n",
    "    _eps = max(1e-3, eps_smooth)  # assuming _eps < lam < 1-_eps\n",
    "    mask_one = (label > _eps).sum(dim=-1)\n",
    "    mix_num = max(mask_one)\n",
    "    mask_one = mask_one >= mix_num\n",
    "    if mask_one.sum() < label.size(0):\n",
    "        pred_one = pred[mask_one==False, :]\n",
    "        label_one = label[mask_one==False, :]\n",
    "        pred = pred[mask_one, :]\n",
    "        label = label[mask_one, :]\n",
    "        weight_one = None\n",
    "        if weight is not None:\n",
    "            weight_one = weight[mask_one==False, ...].float()\n",
    "            weight = weight[mask_one, ...].float()\n",
    "        if verbose: print(f\"pred_one: {mask_one=} {pred_one=}\")\n",
    "    else:\n",
    "        if weight is not None:\n",
    "            weight = weight.float()\n",
    "        pred_one, label_one, weight_one = None, None, None\n",
    "        if verbose: print(f\"no pred_one {mask_one=}\")\n",
    "    # step 2: select k-mixup for the local and global\n",
    "    bs, cls_num = label.size()  # N, C\n",
    "    assert isinstance(eta_weight, list)\n",
    "    # local: between k classes\n",
    "    mask_lam_k = label > _eps  # [N, N], top k is true\n",
    "    lam_k = label[0, label[0, :] > _eps]  # [k,] k-mix relevant classes\n",
    "\n",
    "    # local: original mixup CE loss between C classes\n",
    "    loss = -label * F.log_softmax(pred, dim=-1)  # [N, N]\n",
    "    if class_weight is not None:\n",
    "        loss *= class_weight\n",
    "    loss = loss.sum(dim=-1)  # reduce class\n",
    "\n",
    "    # global: between lam_i and C-k classes\n",
    "    if len(set(lam_k.cpu().numpy())) == lam_k.size(0) and lam_k.size(0) > 1:\n",
    "        if verbose: print(\"calculating global loss for\", lam_k, 'loss so far', loss)\n",
    "        # *** trivial solution: lam=0.5, lam=1.0 ***\n",
    "        assert len(eta_weight) == lam_k.size(0), \\\n",
    "            \"eta weight={}, lam_k={}\".format(eta_weight, lam_k)\n",
    "        for i in range(lam_k.size(0)):\n",
    "            # selected (C-k+1), except lam_k[j], where j!=i (k-1)\n",
    "            mask_lam_i = (label == lam_k[i]) | ~mask_lam_k  # [N, N]\n",
    "            pred_lam_i  = pred.reshape([1, bs, -1])[:, mask_lam_i].reshape(\n",
    "                [-1, cls_num+1-lam_k.size(0)])  # [N, C-k+1]\n",
    "            label_lam_i = label.reshape([1, bs, -1])[:, mask_lam_i].reshape(\n",
    "                [-1, cls_num+1-lam_k.size(0)])  # [N, C-k+1]\n",
    "            # convert to onehot\n",
    "            label_lam_i = (label_lam_i > 0).type(torch.float)\n",
    "            # element-wise losses\n",
    "            loss_global = -label_lam_i * F.log_softmax(pred_lam_i, dim=-1)  # [N, C-1]\n",
    "            if class_weight is not None:\n",
    "                loss_global *= class_weight\n",
    "            # eta reweight\n",
    "            if verbose: print(f\"global loss: {loss_global.sum(dim=-1)} for {lam_k[i]}\")\n",
    "            loss += eta_weight[i] * loss_global.sum(dim=-1)  # reduce class\n",
    "    # apply weight and do the reduction\n",
    "    loss = weight_reduce_loss(\n",
    "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    # step 3: original soft CE loss\n",
    "    if label_one is not None:\n",
    "        loss_one = -label_one * F.log_softmax(pred_one, dim=-1)\n",
    "        if class_weight is not None:\n",
    "            loss_one *= class_weight\n",
    "        loss_one = loss_one.sum(dim=-1)  # reduce class\n",
    "        if verbose: print(f\"loss_one: {loss_one=} {loss=}\")\n",
    "\n",
    "        loss_one = weight_reduce_loss(\n",
    "            loss_one, weight=weight_one, reduction=reduction, avg_factor=avg_factor)\n",
    "        loss += loss_one or 0.0\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmce(preds, r, eta=0.1, verbose=False, **kw): \n",
    "    mce = F.cross_entropy(preds, r.mixed_y, **kw) \n",
    "    dmce1 = ce_masked(preds, r.y1, r.y2, **kw)\n",
    "    dmce2 = ce_masked(preds, r.y2, r.y1, **kw)\n",
    "    if verbose: print(f\"{mce} + {eta}*{dmce1} + {eta}*{dmce2}\")\n",
    "    return (mce + eta*dmce1 + eta*dmce2)\n",
    "\n",
    "def dmce_c(preds, r, eta=[0.1,0.1], verbose=False, **kw):\n",
    "    return soft_mix_cross_entropy(preds, r.mixed_y, eta_weight=eta, verbose=verbose)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed.set_new(1)\n",
    "b = 20\n",
    "a = 11.\n",
    "x1 = tensor([[1,  1, a, 1],\n",
    "             [a,  1, 1, 1]]).float()\n",
    "x2 = tensor([[1,  b, 1, 1],\n",
    "             [1,  1, 1, b]]).float()\n",
    "y1 = (x1).argmax(-1)\n",
    "y2 = (x2).argmax(-1) \n",
    "\n",
    "print(y1, y2)\n",
    "r = mixup_data((x1,y1), (x2,y2), classes=4, permute_1=False, sampler=lambda x: torch.tensor([0.3, 1.0]))\n",
    "\n",
    "dmce_c(r.mixed_x, r, verbose=True), dmce(r.mixed_x, r, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mce(preds, r, eta=None,**kw): return F.cross_entropy(preds, r.mixed_y,**kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp4CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUp4CB(TrainCB):\n",
    "    def __init__(self,alpha=0.4, use_prev=False, eta=0.1, per_batch=False, loss_func=dmce, **kw): \n",
    "        super().__init__(**kw)\n",
    "        self.alpha = alpha\n",
    "        self.prev = None\n",
    "        self.use_prev = use_prev\n",
    "        self.eta = eta\n",
    "        self.dist = torch.distributions.Beta(self.alpha,self.alpha) \n",
    "        self.per_batch = per_batch\n",
    "        self.loss_func=loss_func\n",
    "        \n",
    "    def before_epoch(self, learn):\n",
    "        self.prev = None\n",
    "            \n",
    "    def sample(self, shape): \n",
    "        if self.per_batch: return self.dist.sample([1])\n",
    "        return self.dist.sample(shape)\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        if learn.training and self.alpha is not None: \n",
    "            r = mixup_data(learn.batch, self.prev, sampler=self.sample)\n",
    "            if self.use_prev: self.prev = learn.batch\n",
    "            learn.mixup = r\n",
    "            learn.batch = r.mixed_x, r.mixed_y.argmax(-1)\n",
    "            \n",
    "    def get_loss(self, learn):\n",
    "        if learn.training and self.alpha is not None:\n",
    "            learn.loss = self.loss_func(learn.preds, learn.mixup, eta=self.eta)\n",
    "        else:\n",
    "            super().get_loss(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUpCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data_old(x, y, sampler=torch.distributions.Beta(tensor(1), tensor(1)).sample):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    lam = sampler([1]).to(x.device)\n",
    "    batch_size = x.shape[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, (y_a, y_b, lam)\n",
    "\n",
    "def mixup_criterion_old(lf, pred, my):\n",
    "    y_a, y_b, lam = my\n",
    "    return lam * lf(pred, y_a) + (1 - lam) * lf(pred, y_b)\n",
    "\n",
    "class MixUpCB(TrainCB):\n",
    "    def __init__(self,alpha=0.4, mix_data=mixup_data_old, mix_loss=mixup_criterion_old,**kw): \n",
    "        super().__init__(**kw)\n",
    "        self.alpha = alpha\n",
    "        self.dist = torch.distributions.Beta(self.alpha,self.alpha)\n",
    "        self.mix_data = mix_data\n",
    "        self.mix_loss = mix_loss\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        self.base_lf = learn.loss_func\n",
    "        \n",
    "    def sample(self, n): return self.dist.sample(n)\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        if learn.training and self.alpha is not None: \n",
    "            bx, mixup = self.mix_data(*learn.batch, self.sample)\n",
    "            learn.batch = bx, learn.batch[1]\n",
    "            learn.mixup = mixup\n",
    "\n",
    "    def get_loss(self, learn):\n",
    "        if learn.training and self.alpha is not None:\n",
    "            learn.loss = self.mix_loss(learn.loss_func, learn.preds, learn.mixup) # todo  *learn.batch[self.n_inp:]   \n",
    "        else:\n",
    "            super().get_loss(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbl check \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.previous()\n",
    "model = get_model9(act_gr, norm=nn.BatchNorm2d)\n",
    "print(torch.randn([3]))\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(),rng, metrics, ProgressCB(plot=True)] \n",
    "act_gr = nn.SiLU\n",
    "iw = partial(init_weights, leaky=0.0003)\n",
    "epochs = 1\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), MixUpCB(0.4)] \n",
    "print(torch.randn([3]))\n",
    "model = model.apply(iw)\n",
    "learn2 = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "print(torch.randn([3]))\n",
    "print(next(iter(learn2.dls.train))[1])\n",
    "learn2.fit(epochs, cbs=TimeItCB())\n",
    "print(learn2.model(xb.to('cuda')).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.previous()\n",
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=1e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=False, eta=None, per_batch=True, loss_func=mce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sample??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have weight decay !!!\n",
    "```\n",
    "AdamW (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    base_momentum: 0.85\n",
    "    betas: (0.9499489264325014, 0.999)\n",
    "    capturable: False\n",
    "    eps: 1e-08\n",
    "    foreach: None\n",
    "    initial_lr: 0.0004\n",
    "    lr: 0.0004049030624798597\n",
    "    max_lr: 0.01\n",
    "    max_momentum: 0.95\n",
    "    maximize: False\n",
    "    min_lr: 4e-08\n",
    "    weight_decay: 0.01\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmpOneBatchCB(Callback):\n",
    "    def __init__(self, batches=1):self.batches=batches\n",
    "    def after_batch(self, learn):\n",
    "        learn.batches = getattr(learn, 'batches', self.batches)\n",
    "        #print(\"y:\",learn.batch[1])\n",
    "        print(\"loss:\",learn.loss.item(), \"iter\", learn.iter)\n",
    "        if learn.iter+1 == self.batches:\n",
    "            raise CancelFitException()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn1.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "learn1 = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=1e-2, epochs=1, \n",
    "    cbs=[CmpOneBatchCB()], \n",
    "    train_cb=MixUpCB(0.4), fit=True)\n",
    "#print('lam', learn1.mixup[-1])\n",
    "print(learn1.model(xb.to('cuda')).mean().item())\n",
    "A=torch.get_rng_state().clone()\n",
    "print(torch.randn([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "learn1 = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=1e-2, epochs=1, \n",
    "    cbs=[CmpOneBatchCB()], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=False, eta=None, per_batch=True, loss_func=mce), fit=True)\n",
    "print('lam', learn1.mixup.lam)\n",
    "print(learn1.model(xb.to('cuda')).mean().item())\n",
    "B=torch.get_rng_state().clone()\n",
    "print(torch.randn([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A == B).sum(), A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "learn1 = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=1e-2, epochs=1, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=False, eta=None, per_batch=True, loss_func=mce), dls=dls, fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn3 = run(\n",
    "#     get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "#     base_lr=1e-2, epochs=1, \n",
    "#     cbs=[], \n",
    "#     train_cb=MixUpCB(0.4), dls=dls, fit=False)\n",
    "# epochs = 1\n",
    "# lr = 1e-2\n",
    "# tmax = epochs * len(dls.train)\n",
    "# sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "# xtra = [BatchSchedCB(sched), MixUpCB(0.4)] \n",
    "\n",
    "# learn3.cbs = cbs+xtra\n",
    "# print(learn1.cbs)\n",
    "# learn3.fit(1)\n",
    "# print(learn3.model(xb).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn1.batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn2.batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB()] \n",
    "act_gr = nn.SiLU\n",
    "iw = partial(init_weights, leaky=0.0003)\n",
    "epochs = 5\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), MixUp4CB(0.4, use_prev=False, eta=None, per_batch=True, loss_func=mce)] \n",
    "model = get_model9(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs, cbs=TimeItCB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [93.8] MCE, single_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=2e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=True, eta=None, per_batch=True, loss_func=mce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [92.1] DMCE copy, single_lam eta 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=2e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=True, eta=[0.1,0.1], per_batch=True, loss_func=dmce_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [93.8] DMCE (our), single_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=2e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=True, eta=0.1, per_batch=True, loss_func=dmce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [92.6] DMCE copy, single_lam eta[0.1,0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=2e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=True, eta=[0.1,0.9], per_batch=True, loss_func=dmce_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [93.9] DCME (our), mlam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run(\n",
    "    get_model9(nn.SiLU, norm=nn.BatchNorm2d), leaky=0.0003, \n",
    "    base_lr=2e-2, epochs=5, \n",
    "    cbs=[], \n",
    "    train_cb=MixUp4CB(0.4, use_prev=True, eta=0.1, per_batch=False, loss_func=dmce))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1930ad4244ff4add98b2824f436d4cbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "455e06ae1673469e983dcb5613cfabf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f746a7fc9ff4528bce8936825eaaa5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59c5fe4eed9743a4893acfbfbcf87ed5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63c555524ffa4c4491e43ee5aaa55615": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2eec7fbedb4920afa640e39e518cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_455e06ae1673469e983dcb5613cfabf9",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf9ac8d1d1754cf197ce246206c63f2e",
      "value": 2
     }
    },
    "6ee004a86352495682e8cdd2d4f76408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f88307ff0a849fe918f4c90d7921d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59c5fe4eed9743a4893acfbfbcf87ed5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4f746a7fc9ff4528bce8936825eaaa5d",
      "value": " 2/2 [00:00&lt;00:00, 78.33it/s]"
     }
    },
    "a5425c6fb41c43948a0070d091c47160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9fabe3bf89c45578422ce72800f251b",
       "IPY_MODEL_6d2eec7fbedb4920afa640e39e518cfb",
       "IPY_MODEL_8f88307ff0a849fe918f4c90d7921d59"
      ],
      "layout": "IPY_MODEL_1930ad4244ff4add98b2824f436d4cbf"
     }
    },
    "bf9ac8d1d1754cf197ce246206c63f2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9fabe3bf89c45578422ce72800f251b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63c555524ffa4c4491e43ee5aaa55615",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6ee004a86352495682e8cdd2d4f76408",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

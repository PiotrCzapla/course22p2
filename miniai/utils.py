# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_utils.ipynb.

# %% auto 0
__all__ = ['ReseedCB', 'TimeItCB', 'cache_dataset_as_dict']

# %% ../nbs/12_utils.ipynb 1
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder


from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .mps import *

# %% ../nbs/12_utils.ipynb 3
class ReseedCB(Callback):
    order = 0
    def __init__(self): self.gen, self.new_seed = torch.Generator(), None
    def before_fit(self, learn): 
        self.set_seed(self.new_seed)
        print('Using seed', self.new_seed)
    def set_seed(self, seed=None): 
        if seed is None: 
            self.new_seed = torch.randint(2**31, [1], generator=self.gen).item()
            print(f"New seed: {self.new_seed}")
        else:
            self.new_seed = seed
        set_seed(self.new_seed)
        return self.new_seed
    def previous(self): self.set_seed(self.new_seed)
    def new(self): self.set_seed()

# %% ../nbs/12_utils.ipynb 12
import time

def _format_stats(stats):
    return f"{stats.mean():.2f} Â± {stats.std():.2f}"

class TimeItCB(Callback):
    def __init__(self):
        self.reset()
    def tick(self): return time.time()
         
    def reset(self):
        self.start = self.tick()
        self.setup = None
        self.batches = {True:[], False:[]}
        self.epochs = {True:[], False:[]}
        self.samples = {True:[], False:[]}
    def before_fit(self, learn):
        self.reset()
        
    def before_batch(self, learn):
        if self.setup is None: self.setup = self.tick() - self.start
        self.batches[learn.training].append(self.tick())
    
    def after_batch(self, learn):
        self.batches[learn.training][-1] = self.tick() - self.batches[learn.training][-1]
        self.samples[learn.training].append(learn.batch[0].shape[0])
    
    def before_epoch(self, learn):
        self.epochs[learn.training].append(self.tick())
    
    def after_epoch(self, learn):
        self.epochs[learn.training][-1] = self.tick() - self.epochs[learn.training][-1]
    
    def after_fit(self, learn):
        self.total = self.tick() - self.start
        self.setup = self.setup / self.total
        self.batches = {k:np.array(v) for k,v in self.batches.items()}
        self.epochs = {k:np.array(v) for k,v in self.epochs.items()}
        self.samples = {k:np.array(v) for k,v in self.samples.items()}
        self.print_stats()
    
    def print_stats(self): 
        print(f"Fit {len(self.epochs[True])} in: {self.total:.2f}s, setup: {self.setup:.2f}s, {_format_stats(self.epochs[True])}s per epoch, {_format_stats(self.batches[True])}s per batch")
        #print(f" samples/sec: {self.samples[True].sum() / self.total:.2f}s, time_in_val: {self.epochs[False].sum() / self.total:.2f}s")

# %% ../nbs/12_utils.ipynb 13
# if spawn is not possible, we can cache the entire dataset in memory after transformations
def _with_features(ds):
    setattr((l:=fc.L(ds)), 'features', ds.features)
    return l 
def cache_dataset_as_dict(dd): return {dsn: _with_features(ds) for dsn,ds in dd.items()}

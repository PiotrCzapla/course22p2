# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_utils.ipynb.

# %% auto 0
__all__ = ['ReseedCB', 'RNG', 'rng_seed', 'RNGSeedCB', 'LazyMean', 'LazyMetricsCB', 'LazyProgressCB', 'TimeItCB',
           'cache_dataset_as_dict']

# %% ../nbs/12_utils.ipynb 1
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder


from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .mps import *

# %% ../nbs/12_utils.ipynb 3
class RNGSeedCB(Callback):
    order = 0
    def __init__(self, verbose=True): 
        self.gen, self.new_seed, self.log = torch.Generator(), None, print if verbose else fc.noop
    def before_fit(self, learn): 
        self.set_seed(self.new_seed)
        
    def set_seed(self, seed=None): 
        if seed is None: 
            self.new_seed = torch.randint(2**31, [1], generator=self.gen).item()
        else:
            self.new_seed = seed  
        self.log("Reseed:",self.new_seed)
        set_seed(self.new_seed)
        return self.new_seed
    def new(self): self.set_seed()
    def previous(self): self.set_seed(self.new_seed)
    
ReseedCB = RNGSeedCB
RNG = RNGSeedCB()
rng_seed = RNG # old name

# %% ../nbs/12_utils.ipynb 14
from torcheval.metrics import Metric, Mean
class LazyMean(Mean):
    def __init__(self): 
        super().__init__()
        self.buffer = []
        
    def reset(self):
        super().reset()
        self.buffer = []
    
    def to(self, device): pass # ignore we compute on cpu

    def update(self, val, weight=tensor(1.0)): self.buffer.append((val.detach(), tensor(weight).detach().float()))

    def compute(self):
        for val,w in self.buffer: super().update(val.to('cpu'), weight=w.to('cpu'))
        self.buffer = []    
        return super().compute() 

# %% ../nbs/12_utils.ipynb 15
from copy import copy
from .learner import Mean, master_bar, progress_bar
class LazyMetricsCB(Callback):
    order = MetricsCB.order
    def __init__(self, *ms, device=def_device, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = LazyMean() if torch.backends.mps.is_available() else Mean()
        self.device = def_device

    def _log(self, d): print(d)
    def before_fit(self, learn): 
        learn.metrics = self 
        self.device = [*(cb.device for cb in learn.cbs if isinstance(cb, DeviceCB)), self.device][0]
                
    def before_epoch(self, learn): 
        for o in self.all_metrics.values(): 
            o.reset()
            o.to(self.device)
    
    @torch.no_grad()
    def after_epoch(self, learn):
        log = {k:f'{to_cpu(v.compute()):.3f}' for k,v in self.all_metrics.items()}
        log['epoch'] = learn.epoch
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)
    
    @torch.no_grad()
    def after_batch(self, learn):
        x,y,*_ = learn.batch
        for m in self.metrics.values(): m.update(learn.preds, y)
        self.loss.update(learn.loss, weight=len(x))


# %% ../nbs/12_utils.ipynb 16
class LazyProgressCB(Callback):
    order = MetricsCB.order+1
    def __init__(self, plot=False): 
        self.plot = plot
        self.count = 0
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.dev_losses = []
        self.losses = []

    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)
        if self.plot:
            for l in self.dev_losses: self.losses.append(to_cpu(l))
            self.dev_losses=[]
            #print(self.losses[:10])
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])
        
    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
    def after_batch(self, learn):
        #learn.dl.comment = f'{learn.loss:.3f}' # not sure how it is being used
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.dev_losses.append(learn.loss.detach())

# %% ../nbs/12_utils.ipynb 17
import time

def _format_stats(stats):
    return f"{stats.mean():.2f} Â± {stats.std():.2f}"

class TimeItCB(Callback):
    def __init__(self):
        self.reset()
    def tick(self): return time.time()
         
    def reset(self):
        self.start = self.tick()
        self.setup = None
        self.batches = {True:[], False:[]}
        self.epochs = {True:[], False:[]}
        self.samples = {True:[], False:[]}
    def before_fit(self, learn):
        self.reset()
        
    def before_batch(self, learn):
        if self.setup is None: self.setup = self.tick() - self.start
        self.batches[learn.training].append(self.tick())
    
    def after_batch(self, learn):
        self.batches[learn.training][-1] = self.tick() - self.batches[learn.training][-1]
        self.samples[learn.training].append(learn.batch[0].shape[0])
    
    def before_epoch(self, learn):
        self.epochs[learn.training].append(self.tick())
    
    def after_epoch(self, learn):
        self.epochs[learn.training][-1] = self.tick() - self.epochs[learn.training][-1]
    
    def after_fit(self, learn):
        self.total = self.tick() - self.start
        self.setup = self.setup / self.total
        self.batches = {k:np.array(v) for k,v in self.batches.items()}
        self.epochs = {k:np.array(v) for k,v in self.epochs.items()}
        self.samples = {k:np.array(v) for k,v in self.samples.items()}
        self.print_stats()
    
    def print_stats(self): 
        print(f"Fit {len(self.epochs[True])} in: {self.total:.2f}s, setup: {self.setup:.2f}s, {_format_stats(self.epochs[True])}s per epoch, {_format_stats(self.batches[True])}s per batch")
        #print(f" samples/sec: {self.samples[True].sum() / self.total:.2f}s, time_in_val: {self.epochs[False].sum() / self.total:.2f}s")

# %% ../nbs/12_utils.ipynb 18
# if spawn is not possible, we can cache the entire dataset in memory after transformations
def _with_features(ds):
    setattr((l:=fc.L(ds)), 'features', ds.features)
    return l 
def cache_dataset_as_dict(dd): return {dsn: _with_features(ds) for dsn,ds in dd.items()}

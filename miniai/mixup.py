# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/contest/mixup_dmce_int.ipynb.

# %% auto 0
__all__ = ['mce_float', 'mixup_data_int', 'mce_int', 'mixup_data_per_batch', 'mixup_criterion_per_batch', 'MixUpCB', 'ce_masked',
           'dmce', 'dmce_orig']

# %% ../nbs/contest/mixup_dmce_int.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,random
import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder

from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *
from .sgd import *
from .resnet import *
from .augment import *
from types import SimpleNamespace

# %% ../nbs/contest/mixup_dmce_int.ipynb 27
def mce_float(preds, r, eta=None, **kw): return F.cross_entropy(preds, r.mixed_y,**kw)

# %% ../nbs/contest/mixup_dmce_int.ipynb 29
def mixup_data_int(b2, b1=None, sampler=torch.distributions.Beta(tensor(0.5), tensor(0.5)).sample, classes=10, permute_1=True):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    r = SimpleNamespace()    
    if b1 is None: b1=b2 # first batch uses it self
    x1,y1 = b1
    x2,y2 = b2
    
    if x1.shape[0] != x2.shape[0]: x1,y1=x2,y2 # last batch uses it self

    batch_size = x2.shape[0]
    r.lam = sampler([batch_size]).to(x2.device)
    
    # permute prev batch
    if permute_1:
        r.index = torch.randperm(batch_size).to(x2.device)
        x1,y1 = x1[r.index], y1[r.index]
    # x2 + lam*(x1-x2)
    r.mixed_x = torch.lerp(x2, x1, r.lam.reshape(-1,*[1]*(len(x2.shape)-1)))
    r.y1 = y1
    r.y2 = y2
    r.y = torch.cat([y2[:,None],y1[:,None]],dim=1)
    r.top1y=torch.gather(r.y, 1, (r.lam.expand(r.y1.shape) > 0.5).long()[:,None]).squeeze(-1)
    return r


# %% ../nbs/contest/mixup_dmce_int.ipynb 32
def mce_int(preds, r, reduction=True, eta=None, **kw):
    ce = torch.lerp(F.cross_entropy(preds, r.y2, reduction='none', **kw), 
                    F.cross_entropy(preds, r.y1, reduction='none', **kw), r.lam)
    return ce.mean(-1) if reduction else ce

# %% ../nbs/contest/mixup_dmce_int.ipynb 34
def mixup_data_per_batch(b,b2=None, sampler=torch.distributions.Beta(tensor(1), tensor(1)).sample):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    x,y=b
    r = SimpleNamespace()   
    r.lam = sampler([1]).to(x.device)
    batch_size = x.shape[0]
    r.index = torch.randperm(batch_size).to(x.device)
    # this is less stable than lerp
    #r.mixed_x = x + r.lam * (x[r.index, :] - x)
    r.mixed_x=torch.lerp(x, x[r.index, :], r.lam)
    r.y2, r.y1 = y, y[r.index]
    return r

def mixup_criterion_per_batch(pred, r, lf=F.cross_entropy, eta=None, **kw):
    return torch.lerp(lf(pred, r.y2, **kw), lf(pred, r.y1, **kw), r.lam).squeeze()

# %% ../nbs/contest/mixup_dmce_int.ipynb 58
class MixUpCB(TrainCB):
    def __init__(self,alpha=0.4, use_prev=False, eta=0.1, per_batch=False, loss_func=mce_int, label_smoothing=0.0, **kw): 
        super().__init__(**kw)
        self.alpha = alpha or 0.0
        self.dist = torch.distributions.Beta(self.alpha, self.alpha)
        self.prev = None
        self.use_prev = use_prev
        self.eta = eta 
        self.per_batch = per_batch
        self.loss_func = loss_func
        self.label_smoothing = label_smoothing
    def before_epoch(self, learn):
        self.prev = None
            
    def sample(self, shape): 
        if self.per_batch: return self.dist.sample([1])
        return self.dist.sample(shape)
    
    def before_batch(self, learn):
        if learn.training and self.alpha: 
            r = mixup_data_int(learn.batch, self.prev, sampler=self.sample)
            if self.use_prev: self.prev = learn.batch
            learn.mixup = r
            learn.batch = r.mixed_x, getattr(r,'top1y', learn.batch[1])
            
    def get_loss(self, learn):
        if learn.training and self.alpha:
            #import pdb;pdb.set_trace()
            learn.loss = self.loss_func(learn.preds, learn.mixup, eta=self.eta, label_smoothing=self.label_smoothing)
        else:
            super().get_loss(learn)

# %% ../nbs/contest/mixup_dmce_int.ipynb 97
#https://github.com/Westlake-AI/openmixup/blob/c042813ee0af577d365f0e13b13a4c8486d6e8f7/openmixup/models/losses/cross_entropy_loss.py#L83

def _weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):
    """Apply element-wise weight and reduce loss.
    Args:
        loss (Tensor): Element-wise loss tensor.
        weight (Tensor): Element-wise weights.
        reduction (str): Same as built-in losses of PyTorch. Options are "none",
            "mean" and "sum".
        avg_factor (float): Avarage factor when computing the mean of losses.
    Returns:
        Tensor: Processed loss values.
    """
    # if weight is specified, apply element-wise weight
    if weight is not None:
        loss = loss * weight

    reduction_enum = F._Reduction.get_enum(reduction)
    # if avg_factor is not specified, just reduce the loss
    if avg_factor is None:
        # none: 0, elementwise_mean:1, sum: 2
        if reduction_enum == 1:
            loss = loss.mean()
        elif reduction_enum == 2:
            loss = loss.sum()
    else:
        # if reduction is 'mean', then average the loss by avg_factor
        if reduction_enum == 1:
            loss = loss.sum() / avg_factor
        # if reduction is 'none', then do nothing; otherwise raise an error
        elif reduction != 0:
            raise ValueError('avg_factor can not be used with reduction="sum"')
    return loss

def _soft_mix_cross_entropy(pred,
                           label,
                           weight=None,
                           reduction='mean',
                           class_weight=None,
                           avg_factor=None,
                           eta_weight=None,
                           eps_smooth=1e-3,
                           verbose=False,
                           weight_reduce_loss=_weight_reduce_loss,
                           **kwargs):
    r"""Calculate the Soft Decoupled Mixup CrossEntropy loss using softmax
        The label can be float mixup label (class-wise sum to 1, k-mixup, k>=2).
       *** Warnning: this mixup and label-smoothing cannot be set simultaneously ***
    Decoupled Mixup for Data-efficient Learning. In arXiv, 2022.
    <https://arxiv.org/abs/2203.10761>
    Args:
        pred (torch.Tensor): The prediction with shape (N, C), C is the number
            of classes.
        label (torch.Tensor): The gt label of the prediction with shape (N, C).
            When using "mixup", the label can be float (mixup one-hot label).
        weight (torch.Tensor, optional): Sample-wise loss weight.
        reduction (str): The method used to reduce the loss.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
        class_weight (torch.Tensor, optional): The weight for each class with
            shape (C), C is the number of classes. Default None.
        eta_weight (list): Reweight the global loss in mixup cls loss as,
            loss = loss_local + eta_weight[i] * loss_global[i]. Default to None.
        eps_smooth (float): If using label smoothing, we assume eps < lam < 1-eps.
    Returns:
        torch.Tensor: The calculated loss
    """
    # *** Assume k-mixup in C classes, k >= 2 and k << C ***
    # step 1: remove labels have less than k-hot (mixed between the
    #    same class will result in the original onehot)
    _eps = max(1e-3, eps_smooth)  # assuming _eps < lam < 1-_eps
    mask_one = (label > _eps).sum(dim=-1)
    mix_num = max(mask_one)
    mask_one = mask_one >= mix_num
    if mask_one.sum() < label.size(0):
        pred_one = pred[mask_one==False, :]
        label_one = label[mask_one==False, :]
        pred = pred[mask_one, :]
        label = label[mask_one, :]
        weight_one = None
        if weight is not None:
            weight_one = weight[mask_one==False, ...].float()
            weight = weight[mask_one, ...].float()
        if verbose: print(f"pred_one: {mask_one=} {pred_one=}")
    else:
        if weight is not None:
            weight = weight.float()
        pred_one, label_one, weight_one = None, None, None
        if verbose: print(f"no pred_one {mask_one=}")
    # step 2: select k-mixup for the local and global
    bs, cls_num = label.size()  # N, C
    assert isinstance(eta_weight, list)
    # local: between k classes
    mask_lam_k = label > _eps  # [N, N], top k is true
    lam_k = label[0, label[0, :] > _eps]  # [k,] k-mix relevant classes

    # local: original mixup CE loss between C classes
    loss = -label * F.log_softmax(pred, dim=-1)  # [N, N]
    if class_weight is not None:
        loss *= class_weight
    loss = loss.sum(dim=-1)  # reduce class

    # global: between lam_i and C-k classes
    if len(set(lam_k.cpu().numpy())) == lam_k.size(0) and lam_k.size(0) > 1:
        if verbose: print("calculating global loss for", lam_k, 'loss so far', loss)
        # *** trivial solution: lam=0.5, lam=1.0 ***
        assert len(eta_weight) == lam_k.size(0), \
            "eta weight={}, lam_k={}".format(eta_weight, lam_k)
        for i in range(lam_k.size(0)):
            # selected (C-k+1), except lam_k[j], where j!=i (k-1)
            mask_lam_i = (label == lam_k[i]) | ~mask_lam_k  # [N, N]
            pred_lam_i  = pred.reshape([1, bs, -1])[:, mask_lam_i].reshape(
                [-1, cls_num+1-lam_k.size(0)])  # [N, C-k+1]
            label_lam_i = label.reshape([1, bs, -1])[:, mask_lam_i].reshape(
                [-1, cls_num+1-lam_k.size(0)])  # [N, C-k+1]
            # convert to onehot
            label_lam_i = (label_lam_i > 0).type(torch.float)
            # element-wise losses
            loss_global = -label_lam_i * F.log_softmax(pred_lam_i, dim=-1)  # [N, C-1]
            if class_weight is not None:
                loss_global *= class_weight
            # eta reweight
            if verbose: print(f"global loss: {loss_global.sum(dim=-1)} for {lam_k[i]}")
            loss += eta_weight[i] * loss_global.sum(dim=-1)  # reduce class
    # apply weight and do the reduction
    loss = weight_reduce_loss(
        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)

    # step 3: original soft CE loss
    if label_one is not None:
        loss_one = -label_one * F.log_softmax(pred_one, dim=-1)
        if class_weight is not None:
            loss_one *= class_weight
        loss_one = loss_one.sum(dim=-1)  # reduce class
        if verbose: print(f"loss_one: {loss_one=} {loss=}")

        loss_one = weight_reduce_loss(
            loss_one, weight=weight_one, reduction=reduction, avg_factor=avg_factor)
        loss += loss_one or 0.0

    return loss


# %% ../nbs/contest/mixup_dmce_int.ipynb 108
def ce_masked(preds, y, ignored_y, **kwargs):
    """
    y - one hot encoded label
    ingored_y - one hot encloded of ignored class
    """
    N, C = preds.shape
    mask = ignored_y==0
    mpreds = preds[mask].reshape(N, C-1)
    my = y[mask].float().reshape(N, C-1)
    return F.cross_entropy(mpreds, my, reduction='none', **kwargs)

# %% ../nbs/contest/mixup_dmce_int.ipynb 112
def dmce(preds, r, eta=0.1, verbose=False, **kw): 
    mce = mce_int(preds, r, reduction=False, **kw)
    y1 = F.one_hot(r.y1, preds.shape[-1])
    y2 = F.one_hot(r.y2, preds.shape[-1])
    dmce1 = ce_masked(preds, y1, y2, **kw)
    dmce2 = ce_masked(preds, y2, y1, **kw)
    if verbose: print(f"{mce} + {eta}*{dmce1} + {eta}*{dmce2}")
    return (mce + eta*dmce1 + eta*dmce2).mean()

def dmce_orig(preds, r, eta=[0.1,0.1], verbose=False, **kw):
    y1 = F.one_hot(r.y1, preds.shape[-1]).float()
    y2 = F.one_hot(r.y2, preds.shape[-1]).float()
    mixed_y = torch.lerp(y2,y1,r.lam[...,None])
    return _soft_mix_cross_entropy(preds, mixed_y, eta_weight=eta, verbose=verbose)                

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb.

# %% auto 0
__all__ = ['Mish', 'replace_top_losess', 'TopLossesCallback', 'CustomTrainingSampler', 'CustomDataLoader', 'get_model9']

# %% ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,random
import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder

from ..datasets import *
from ..conv import *
from ..learner import *
from ..activations import *
from ..init import *
from ..sgd import *
from ..resnet import *
from ..augment import *
from torchvision import transforms

# %% ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb 7
#Mish - "Mish: A Self Regularized Non-Monotonic Neural Activation Function"
#https://arxiv.org/abs/1908.08681v1
#implemented for PyTorch / FastAI by lessw2020 
#github: https://github.com/lessw2020/mish

class Mish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)
        return x *( torch.tanh(F.softplus(x)))

# %% ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb 10
def replace_top_losess(n, top_losses, prev_idx):
    num_samples = len(prev_idx)
    top_losses = top_losses.cpu()
    prev_idx = prev_idx.cpu()
    easiest_idx = prev_idx[top_losses[-n:]] 
    hardest_idx = prev_idx[top_losses[:n]]
    indices = torch.arange(num_samples)
    indices[easiest_idx] = hardest_idx
    return indices

# %% ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb 12
from typing import Iterator
from torch.utils.data import DataLoader, WeightedRandomSampler

class TopLossesCallback(Callback):
    def __init__(self, n=None):
        self.epoch_preds = []
        self.epoch_targets = []
        self.n = n or {
            0: 0,
            1: 0.21,
            2: 0.42,
            3: 0.21,
            4: 0
        }
        
    def before_fit(self, learn):
        self.epoch_preds = []
        self.epoch_targets = []

    @torch.no_grad()
    def _calculate_top_losses(self):
        preds = torch.cat(self.epoch_preds, dim=0)
        targets = torch.cat(self.epoch_targets, dim=0)
        losses = F.cross_entropy(preds, targets, reduce=False)
        return torch.topk(losses, preds.shape[0]).indices
    
    def after_batch(self, learn):
        if learn.training: 
            self.epoch_preds.append(learn.preds)
            self.epoch_targets.append(learn.batch[1])
            
    def before_epoch(self,learn):
        mix_precentage = self.n.get(learn.epoch,0)
        learn.dls.train.sampler.mix_precentage = self.n.get(learn.epoch,0)
        #print("setting epoch", learn.epoch)
        
    def after_epoch(self, learn):
        if self.epoch_preds:
            #print("after epoch", len(self.epoch_targets))
            learn.dls.train.sampler.top_losses = self._calculate_top_losses()
            self.epoch_preds = []
            self.epoch_targets = []
        
# tweaked from tommyc's version
# Before certain epoch drop a % of the training dataset with the lowest losses.
# Replace them with the the same % of the training dataset with the highest losses.
# This gives the model two opportunities to train on the most challenging images.
class CustomTrainingSampler(WeightedRandomSampler):
    def __init__(self, *args, **kwargs):
        WeightedRandomSampler.__init__(self, *args, **kwargs)
        self.data_indexes_for_epoch = torch.randperm(self.num_samples, generator=self.generator)
        self.top_losses = self.data_indexes_for_epoch.clone()
        self.mix_precentage = 0 

    def setup_epoch(self, mix_precentage=None):
        rand_tensor = torch.randperm(self.num_samples, generator=self.generator)
        n = int((mix_precentage or self.mix_precentage) * self.num_samples)
        if n != 0:
            print(f"Replacing top {n} expamples out of {self.num_samples}")
            new_idx = replace_top_losess(n, self.top_losses, self.data_indexes_for_epoch)
            self.data_indexes_for_epoch = new_idx[rand_tensor]
        else:
            self.data_indexes_for_epoch = rand_tensor

            
    def __iter__(self) -> Iterator[int]:
        self.setup_epoch(None)
        yield from self.data_indexes_for_epoch

class CustomDataLoader:
    def __init__(self, *dls): 
        self.train,self.valid = dls[:2]

    def get_sampler(num_samples, mode="train"):
        if mode != "train": return None
        return CustomTrainingSampler(weights=[1.0]*num_samples, num_samples=num_samples)

    @classmethod
    def from_dd(cls, dd, batch_size, **kwargs):
        return cls(*[DataLoader(ds, batch_size, sampler=cls.get_sampler(len(ds), mode), collate_fn=collate_dict(ds), **kwargs) 
                     for mode, ds in dd.items()])

# %% ../../nbs/contest/ct_curriculum/ct94.9_rerun-speed.ipynb 18
def get_model9(act=nn.ReLU, nfs=(32,288,288,288,288,288), norm=nn.BatchNorm2d):#,256
    layers = [ResBlock(1, 32, ks=5, stride=1, act=act, norm=norm)]
    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]
    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]
    return nn.Sequential(*layers).to(def_device)

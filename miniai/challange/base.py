# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/contest/base_run_resnet18d_958.ipynb.

# %% auto 0
__all__ = ['upscale32', 'upscale', 'setup_fasion_mnist', 'multi_kw', 'run', 'model_base_resnet', 'Upscale', 'timm_model',
           'model9_ct', 'DelayedBatchTransformCB', 'tfm_batch', 'get_augcb', 'MixUpFP16CB']

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,random
import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torchvision import transforms
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torch.optim import lr_scheduler
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder

from ..datasets import *
from ..conv import *
from ..learner import *
from ..activations import *
from ..init import *
from ..sgd import *
from ..resnet import *
from ..augment import *
from .ct94ep5 import *
from ..utils import *
from ..accel import *
import timm

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 5
from fastcore.test import test_close
from torch import distributions

def setup_fasion_mnist(seed=1):
    global RNG
    torch.set_printoptions(precision=8, linewidth=140, sci_mode=False)
    mpl.rcParams['image.cmap'] = 'gray'

    import logging
    logging.disable(logging.WARNING)

    if fc.defaults.cpus>8: fc.defaults.cpus=8

    global RNG
    RNG.set_seed(1)

    xl,yl = 'image','label'
    name = "fashion_mnist"
    xmean,xstd = 0.28, 0.35

    @inplace
    def transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]

    dsd = load_dataset(name)
    tds = dsd.with_transform(transformi)
    tds.cached = cache_dataset_as_dict(tds)
    return tds
    

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 7
import functools
def multi_kw(f):
    @functools.wraps(f)
    def kw(*dicts, **kwargs):
        r = {}
        for d in (*dicts, kwargs):
            for k, v in d.items():
                if k in r and isinstance(r, (list, tuple)): 
                    r[k] = [*r[k], *v]
                elif k not in r:
                    r[k] = v
                else:
                    print(f"duplicate key {k} ")
        return f(**r)
    return kw

@multi_kw
def run(model, leaky=0.0, m=1, cbs=tuple(), fit=True, train_cb=MixedPrecision(), epochs=5, base_lr=2e-2, 
        loss_func=F.cross_entropy, bs=256, tta=False, dls=None, verbose=False, 
        sched_fn=lr_scheduler.OneCycleLR, opt_func=optim.AdamW, tds=None):
    from miniai.utils import RNG
    metrics = LazyMetricsCB(accuracy=MulticlassAccuracy())
    iw = partial(init_weights, leaky=leaky) if leaky is not None else fc.noop
    lr = base_lr*m
    print("Batch size", bs*m)
    dls = dls or DataLoaders.from_dd(tds, bs*m, num_workers=0) 
    tmax = epochs * len(dls.train)
    sched = partial(sched_fn, max_lr=lr, total_steps=tmax)

    cbs = [DeviceCB(), RNG, metrics, BatchSchedCB(sched), *cbs, train_cb] 
    learn = Learner(model.apply(iw), dls, loss_func, lr=lr, cbs=cbs, opt_func=opt_func)
    learn.train_cb = train_cb
    if verbose: 
        print(torch.randn([3]))
        print(next(iter(learn.dls.train))[1])
    if fit:
        learn.fit(epochs, cbs=[TimeItCB(), LazyProgressCB(plot=epochs>1)])
    if tta:
        ## TTA
        ap1, at = learn.capture_preds()
        ttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)
        ap2, at = learn.capture_preds(cbs=[ttacb])
        ap = torch.stack([ap1,ap2]).mean(0).argmax(1)
        print('TTA:', round((ap==at).float().mean().item(), 4))
    return learn

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 12
# The last resnet model 92.8% 5epochs
def model_base_resnet():
    def get_model(act=nn.ReLU, nfs=(8,16,32,64,128,256), norm=nn.BatchNorm2d):
        layers = [ResBlock(1, 8, stride=1, act=act, norm=norm)]
        layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]
        layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]
        return nn.Sequential(*layers).to(def_device)
    RNG.previous()
    act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)
    
    return dict(model=get_model(act_gr, norm=nn.BatchNorm2d), leaky=0.1)

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 16
class Upscale:
    def __init__(self, sz, align=True, const=0): fc.store_attr() 
    def _sz(self, n): return round(self.sz*n / 32)*32 if self.align else self.sz*n
    def antialiased(self, n): 
        return transforms.Resize(self._sz(n), transforms.InterpolationMode.BILINEAR, antialias=True)
    def bilinear(self, n): 
        return transforms.Resize(self._sz(n), transforms.InterpolationMode.BILINEAR, antialias=False)
    def bicubica(self, n): 
        return transforms.Resize(self._sz(n), transforms.InterpolationMode.BICUBIC, antialias=True)
    def bicubic(self, n): 
        return transforms.Resize(self._sz(n), transforms.InterpolationMode.BICUBIC, antialias=False)
    def nearest(self, n): 
        return transforms.Resize(self._sz(n), transforms.InterpolationMode.NEAREST)
    def pad(self, n): 
        return transforms.Pad((self._sz(n)-self.sz)//2, padding_mode='constant', fill=self.const)

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 17
upscale32 = Upscale(28, align=True, const=-0.800000011920929)
upscale = Upscale(28, align=False, const=-0.800000011920929)

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 29
def timm_model(name, resize=nn.Identity(), leaky=0.0, **kw):
    """ Timm model with no pretraining and upscaling, resnet18d trains at 94.2% 5ep, noaug"""
    RNG.previous()
    props = dict(in_chans=1, num_classes=10, pretrained=False)
    props.update(kw)
    model = nn.Sequential( 
        resize,
        timm.create_model(name, **props)
    )
    return dict(model=model, leaky=leaky)

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 30
def model9_ct(**kw):
    """ Cristopher Thomas 94.9% 5epochs, trains at 94.2% 5ep, noaug, 1ep 0.913 in 17s (vs 7s resnet)"""
    from miniai.challange.ct94ep5 import get_model9
    RNG.previous()
    return dict(model=get_model9(Mish, norm=nn.BatchNorm2d), base_lr=1e-2, leaky=0.0003) | kw

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 53
class DelayedBatchTransformCB(BatchTransformCB):
    def __init__(self, *args,epoch=0,**kwargs): 
        super().__init__(*args, **kwargs)
        self.epoch=epoch
    def before_batch(self, learn):
        if self.epoch <= learn.epoch: 
            super().before_batch(learn)
        
def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])
def get_augcb(*ag, delay=0):
    tfms = nn.Sequential(*ag)
    return dict(cbs=[DelayedBatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False, epoch=delay)])

# %% ../../nbs/contest/base_run_resnet18d_958.ipynb 100
from ..mixup import MixUpCB
class MixUpFP16CB(MixUpCB, AccelerateCB): pass
